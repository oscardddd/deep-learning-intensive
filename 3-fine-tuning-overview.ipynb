{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4849f3-51e1-4299-9f55-9c70c1f1c431",
   "metadata": {},
   "source": [
    "# Fine Tuning Models - Part I\n",
    "> An introduction to fine-tuning using built-in datasets\n",
    "\n",
    "In this notebook, we'll use: https://huggingface.co/transformers/training.html# as a guide for our work.  The notebook cells are copied here and our job is to figure out what's going on in each of the cells.  To show this, we'll fill in each of the headers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abbc1b7-f4b4-4cdd-93ed-ce92742fe4c6",
   "metadata": {},
   "source": [
    "# 1.  ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067752c-e281-4ffe-9140-828d158751a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers\n",
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b4f46c-e1c0-4abc-9498-a45211bcf9a6",
   "metadata": {},
   "source": [
    "# 2. ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa3949-c286-4a82-af4f-c5f7594a6c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a371ec-98f5-4be7-b085-a2eed2f9c3d3",
   "metadata": {},
   "source": [
    "# 3. ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b6693-573c-4515-b3bc-0a70c43945c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"imdb\")\n",
    "type(raw_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d558ecf4-e35b-4afa-a1aa-ca16e86ab432",
   "metadata": {},
   "source": [
    "# 4. ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a7940-ae31-4efe-9398-93da6b2b0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71deff6d-3c8a-4bfc-9b5c-c532ef5cdfd7",
   "metadata": {},
   "source": [
    "# 5. ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d994af-196d-4dba-8c1b-95a75d8fa14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08845526-78fb-4096-bb4b-1ca13c7eda12",
   "metadata": {},
   "source": [
    "# 6. ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee10d74e-2496-4c62-bd3f-8f0786a8aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "full_train_dataset = tokenized_datasets[\"train\"]\n",
    "full_eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da664a0-b63c-450a-ae74-bab101d55ea1",
   "metadata": {},
   "source": [
    "# 7. ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606be9d-e38c-48dd-9a8b-73be81ab5d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "model.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf580c86-c17e-47d2-b63a-39df13c0d373",
   "metadata": {},
   "source": [
    "# 8. ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa52fcb-33c9-4a6f-a165-719e19b19f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\"test_trainer\")\n",
    "training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7286fc86-ecde-470c-a605-2e467c1b92e8",
   "metadata": {},
   "source": [
    "# 9. ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aa3713-39b5-4b88-8d41-e2fe41a72ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8598b19-507a-4594-a772-cba9e2bb6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc468b0-0f24-42aa-8642-3e8207394f42",
   "metadata": {},
   "source": [
    "# 10. ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53139608-67f6-417b-9d9c-9218c2f228f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b28b3-3eaa-438c-92b4-8931485b273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
