{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and pandas for deep learning\n",
    "> Using Python and pandas for working with your files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we examine how one can work with files, particularly with pandas to get the data in a desired format for usage with deep learning packages.  Here, we'll examine how to use Google Colab with Google Drive mounted so that you'll be able to access your data files.  Note that there are many ways to achieve this; one method is described below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mounting with Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instructions for mounting Google Drive are concisely described in the 0 notebook.  For more options about uploading and interacting with Google Colab, see their IO notebook here: https://colab.research.google.com/notebooks/io.ipynb .\n",
    "\n",
    "Here, the approach we will take is as follows.  First, we'll mount Google Drive via https://colab.research.google.com .  So we'll all have access to the same files, we'll go ahead and clone this repository in Google Drive.  This will give us access to the repository and file contents programmatically.  Note that when you first mount, you may have to enter some credentials and provide some approvals.  The steps are outlined here.\n",
    "\n",
    "## 1.  Mount Google Drive\n",
    "Note that you can also use the mount button in the sidebar.  Follow all steps to provide approval and access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Clone the repo\n",
    "Again, we're cloning the repo mostly just so we'll have the desired files that we want and you can see how to do it using Google Drive.  Here, we'll simulate this by cloning within your Google Drive.  You can see this with the `cd` (Change Directory) command below.\n",
    "\n",
    "Again, keep in mind that if your data already exists somewhere on your drive, you wouldn't necessarily have to do this unless you wanted to access the repo on your drive.  You would use the filepaths directly as shown in a following step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd drive/MyDrive\n",
    "!git clone https://github.com/vanderbilt-data-science/deep-learning-intensive.git\n",
    "%cd deep-learning-intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install and load the required modules\n",
    "We need to pip install transformers.  We'll do this below.  The rest are already available through Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data science packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#import file helper packages\n",
    "import glob\n",
    "\n",
    "#dl imports\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example scenario\n",
    "You're working with the text of number of magazine articles, written by a number of authors, whose names are not included in the text.  The magazine articles have unique IDs, which are given by their filenames.  \n",
    "\n",
    "You also have an accompanying CSV file with details about all of the authors, including their name, age, years of employment as a journalist, college major, and IDs corresponding to the articles that they've written.  \n",
    "\n",
    "You're interested in learning whether their college major affects the sentiments of their articles.  The only data that you have access to is gathered on a particular day over a set of journalists and articles.  You've decided that the first crack you'll take about understanding this relationship is to separate the journalists (or articles) into groups according to their college major, and count the sentiments for all the corresponding articles for each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access data\n",
    "The first thing you'll need to do is access your data and be able to load it for processing with Python.  We'll use `glob` for this.  Note that we've changed into the repo directory, meaning that the filepath to `workshop-files` is `workshop-files` with no other additional paths necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob('workshop-files/*.txt')\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get file contents\n",
    "We're going to be using the requests package to read from a remote file (the repo on GitHub).  You can use the `requests.get(insert-url-path-here.txt)` function to get the contents of a remote url.  Let's use a list comprehension to implement this functionality.  We'll do this together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file contents\n",
    "file_contents = []\n",
    "for file in filenames:\n",
    "    with open(file, 'r') as f:\n",
    "        file_contents.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of file contents list:', len(file_contents))\n",
    "file_contents[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read tabular data\n",
    "Here, we'll read the tabular data stored in the csv about the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate file path\n",
    "author_df = pd.read_csv('workshop-files/author_data.csv')\n",
    "author_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframes from lists/dictionaries\n",
    "Now, we'll make a dataframe of the files and concatenate them to the df_list.  Note that the split character below `/` may need to be changed to `\\\\` on windows machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, create a dataframe from your filename list and your text\n",
    "tinfo_df = pd.DataFrame({'filename':[fname.split('/')[-1] for fname in filenames], 'text':file_contents})\n",
    "tinfo_df['article_id'] = tinfo_df['filename'].apply(lambda x: int(x.split('.')[0]))\n",
    "tinfo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting data together: joining data\n",
    "Now, we can join the data together.  There are many different types of joins, but essentially a join will look for one or more key, match the key, and then bind the columns of both dataframes together where the key matches.  We'll do a simple inner join here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Text info columns:\", tinfo_df.columns.tolist())\n",
    "print(\"Author columns:\", author_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join tables\n",
    "full_df = pd.merge(author_df, tinfo_df, on='article_id')\n",
    "\n",
    "#print some info\n",
    "display(full_df.head())\n",
    "print(full_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Results\n",
    "Now that we have all of our data together and are able to extract the text of any desired subset, let's do some modeling!  Let's use a sentiment analysis pipeline to get the results.\n",
    "### In-class exercise\n",
    "Using the text classification pipeline, pass in either the list of texts or iterate through the texts to get infererences on each of the article texts.  Fill in the cell below with the required code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get modeling results\n",
    "#sentiment_classifier = add your code here and uncomment\n",
    "#sent_results = add your code here and uncomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe from list of dictionaries\n",
    "sent_df = pd.DataFrame(sent_results)\n",
    "sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate dataframes\n",
    "final_df = pd.concat([author_df, sent_df], axis=1)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's actually answer our question!\n",
    "Can we see a difference between the sentiments of articles based on college major?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.groupby(['college major'])['label'].value_counts().unstack().transpose().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we wanted to compute an aggregate score?\n",
    "Looking at negative and positive is a bit hard.  Maybe we could just calculate a score reflecting whether overall, the group tends to have positive reviews.  We can calculate this as `no_positive_reviews - no_negative_reviews`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negpos_df = final_df.groupby(['college major'])['label'].value_counts().unstack().fillna(0)\n",
    "negpos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negpos_df['overall_positivity'] = negpos_df['POSITIVE'] - negpos_df['NEGATIVE']\n",
    "negpos_df['overall_positivity'].transpose().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What we've covered\n",
    "We've covered a lot of ground today!  We've discussed several things:\n",
    "* Reading in text files via Google Colab and locally via the glob package\n",
    "* Example of HuggingFace sentiment-analysis model inference using demo function\n",
    "* Converting outputs to dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework assignment\n",
    "Upload your data to Google Drive if you can, and make sure that you can access it programmatically!  We'll be working on your own data for the next few classes.  If something prevents you from uploading data to cloud storage, make sure you follow the instructions in the `0-Installations` notebook to install Anaconda locally.  Make sure you install HuggingFace transformers and all required Pytorch (or TensorFlow) packages. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
