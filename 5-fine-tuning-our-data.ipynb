{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4849f3-51e1-4299-9f55-9c70c1f1c431",
   "metadata": {},
   "source": [
    "# Fine Tuning Models - Using Custom Data\n",
    "> Fine-tuning using your own data\n",
    "\n",
    "In this notebook, we'll use:https://huggingface.co/transformers/custom_datasets.html as a guide for our work.  The notebook headers mirror the ones of notebook 3.  However, in this notebook, we'll use our own custom data available through our `workshop-files` subdirectory.  Some code has already been provided from Notebook 2.  Other code, we will write together.  See the solutions notebook if you fall behind!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108e35b6-6e7f-4a59-8ed5-9bb9ebbb8e43",
   "metadata": {},
   "source": [
    "# 0. Preliminaries\n",
    "You can use the following code to mount your drive and cd into the relevant directory.  Uncomment the git clone command if you don't have the `deep-learning-intensive` repo already cloned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ae6a5-1b4f-43de-b950-89ddf24269f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29480a8e-de29-4a9c-b700-4ed4b48c5e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd drive/MyDrive\n",
    "#!git clone https://github.com/vanderbilt-data-science/deep-learning-intensive.git\n",
    "%cd deep-learning-intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abbc1b7-f4b4-4cdd-93ed-ce92742fe4c6",
   "metadata": {},
   "source": [
    "# 1.  Installing Required Packages\n",
    "Note that this is mostly required if you're on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067752c-e281-4ffe-9140-828d158751a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers\n",
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b4f46c-e1c0-4abc-9498-a45211bcf9a6",
   "metadata": {},
   "source": [
    "# 2. Importing Packages for Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa3949-c286-4a82-af4f-c5f7594a6c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a371ec-98f5-4be7-b085-a2eed2f9c3d3",
   "metadata": {},
   "source": [
    "# 3. Load Data\n",
    "## Read in data and convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b6693-573c-4515-b3bc-0a70c43945c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get filenames list\n",
    "filenames = glob.glob('workshop-files/*.txt')\n",
    "\n",
    "#read file contents\n",
    "file_contents = []\n",
    "for file in filenames:\n",
    "    with open(file, 'r') as f:\n",
    "        file_contents.append(f.read())\n",
    "\n",
    "#convert to df\n",
    "tinfo_df = pd.DataFrame({'filename':[fname.split('\\\\')[-1] for fname in filenames], 'text':file_contents})\n",
    "tinfo_df['article_id'] = tinfo_df['filename'].apply(lambda x: int(x.split('.')[0]))\n",
    "\n",
    "#read author csv\n",
    "author_df = pd.read_csv('workshop-files/author_data.csv')\n",
    "\n",
    "#join\n",
    "full_df = pd.merge(author_df, tinfo_df, on='article_id')\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a908fd-e893-48cd-9753-d6d3f4b8d6c5",
   "metadata": {},
   "source": [
    "## Add training labels and split column\n",
    "Note that our data currently doesn't have any training labels, so I'll make some up here add concatenate them to the dataframe.  I'll also add a split column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651cce0-60eb-44b8-9ef0-835c27c09196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create training labels\n",
    "label_dict = {0:'elle', 1:'people'}\n",
    "labels = [0]*10 + [1]*10\n",
    "full_df['labels'] = pd.Series(labels).sample(frac=1, random_state=2345).reset_index(drop=True)\n",
    "\n",
    "#create split labels\n",
    "splits = [0]*15 + [1]*5\n",
    "full_df['split'] = pd.Series(splits).sample(frac=1, random_state=2323).reset_index(drop=True)\n",
    "\n",
    "#view\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d558ecf4-e35b-4afa-a1aa-ca16e86ab432",
   "metadata": {},
   "source": [
    "# 4. Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a7940-ae31-4efe-9398-93da6b2b0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tokenizer - you can use the same one as in the previous notebook example\n",
    "#tokenizer = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71deff6d-3c8a-4bfc-9b5c-c532ef5cdfd7",
   "metadata": {},
   "source": [
    "# 5. Tokenize Inputs and Convert to PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d994af-196d-4dba-8c1b-95a75d8fa14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tokenized representations\n",
    "#train_encodings = \n",
    "#val_encodings = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc59e5-52b9-4449-8cda-bd94e88e980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helpers for class size and class names\n",
    "no_classes = len(full_df.query('split==0')['labels'].unique())\n",
    "train_classes = [label_dict[class_ind] for class_ind in range(no_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061e38a-4e26-4294-8a3f-99f3a51de49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create custom Datasets Class\n",
    "class ArticlesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "#Create datasets from encodings\n",
    "#train_dataset = \n",
    "#val_dataset = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08845526-78fb-4096-bb4b-1ca13c7eda12",
   "metadata": {},
   "source": [
    "# 6. Split Data\n",
    "Already done above!  Whoo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da664a0-b63c-450a-ae74-bab101d55ea1",
   "metadata": {},
   "source": [
    "# 7. Create Model for Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606be9d-e38c-48dd-9a8b-73be81ab5d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, you'll create your model for sequence classification using bert.  Pass in two additional parameters:\n",
    "#num_classes (find the appropriate variable with the number of classes in this notebook) and\n",
    "#id2word (this is a dictionary, also in this notebook, that defines the correspondence between labels and their names)\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", ...)\n",
    "#model.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf580c86-c17e-47d2-b63a-39df13c0d373",
   "metadata": {},
   "source": [
    "# 8. Setup arguments for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa52fcb-33c9-4a6f-a165-719e19b19f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\"test_trainer\",\n",
    "                                 logging_strategy='epoch')\n",
    "training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7286fc86-ecde-470c-a605-2e467c1b92e8",
   "metadata": {},
   "source": [
    "# 9. Train model (without computing metrics)\n",
    "We won't run this one - we'll just use the next set of code to train and evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aa3713-39b5-4b88-8d41-e2fe41a72ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer = Trainer(model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8598b19-507a-4594-a772-cba9e2bb6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc468b0-0f24-42aa-8642-3e8207394f42",
   "metadata": {},
   "source": [
    "# 10. Train model using evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53139608-67f6-417b-9d9c-9218c2f228f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b28b3-3eaa-438c-92b4-8931485b273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ba1189-3e5f-42ae-bfae-e79f85a439f5",
   "metadata": {},
   "source": [
    "# 11. Additional Exercises with `Trainer`\n",
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5bd0ec-ec3f-4f0f-b52f-70c19f6a79b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f6ec1-24fb-43f7-a52a-1cdab4861fc5",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83797f24-bcfb-405d-9f3a-01c8b26fd7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad2ce4-1806-4bf3-9b68-6afa33356b82",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a777f-2b74-43b5-ab8f-b8746165d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('bert-magazine-classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94abc5f1-076e-4167-9c85-26d77404797f",
   "metadata": {},
   "source": [
    "## Use as pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9238bf-3e4c-4558-99a9-72fbbf328787",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_classifier = pipeline('text-classification', model='bert-magazine-classifier')\n",
    "mag_classifier(full_df['text'].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
