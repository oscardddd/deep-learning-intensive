{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4849f3-51e1-4299-9f55-9c70c1f1c431",
   "metadata": {},
   "source": [
    "# Fine-Tuning Models\n",
    "> Fine-tuning using your own data\n",
    "\n",
    "In this notebook, we'll use two references:https://huggingface.co/transformers/custom_datasets.html as a guide for our work.  We'll use the HuggingFace dataset we've already created and use it directly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abbc1b7-f4b4-4cdd-93ed-ce92742fe4c6",
   "metadata": {},
   "source": [
    "### Install required packages\n",
    "Note that this is mostly required if you're on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067752c-e281-4ffe-9140-828d158751a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install transformers\n",
    "#! pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b4f46c-e1c0-4abc-9498-a45211bcf9a6",
   "metadata": {},
   "source": [
    "### Import packages of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa3949-c286-4a82-af4f-c5f7594a6c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "from transformers import pipeline\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a371ec-98f5-4be7-b085-a2eed2f9c3d3",
   "metadata": {},
   "source": [
    "# 1. Load data from HuggingFace Hub or from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b6693-573c-4515-b3bc-0a70c43945c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration charreaubell___demo_data-cdb143897cf94e86\n",
      "Reusing dataset parquet (/Users/bellcs1/.cache/huggingface/datasets/parquet/charreaubell___demo_data-cdb143897cf94e86/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bcc54e7a8734ee38f23b70d41e6584c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_path = 'charreaubell/demo_data'\n",
    "demo_ds = load_dataset(ds_path, use_auth_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d558ecf4-e35b-4afa-a1aa-ca16e86ab432",
   "metadata": {},
   "source": [
    "# 2. Pre-process inputs\n",
    "What's a tokenizer and what does it do? Let's learn more using Huggingface's [instruction on tokenizers](https://huggingface.co/course/chapter2/4?fw=pt). Then, let's try it on our own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a7940-ae31-4efe-9398-93da6b2b0aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-cased'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d994af-196d-4dba-8c1b-95a75d8fa14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define tokenizing function\n",
    "def tokenize_inputs(example):\n",
    "    return tokenizer(example['text'], truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc59e5-52b9-4449-8cda-bd94e88e980d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f283aeefc054d6cbfc0fd99aea486d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/bellcs1/.cache/huggingface/datasets/parquet/charreaubell___demo_data-cdb143897cf94e86/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121/cache-e1a35216368d05fc.arrow\n"
     ]
    }
   ],
   "source": [
    "#do the tokenizing using map function\n",
    "tokenized_ds = demo_ds.map(tokenize_inputs, batched=True,\n",
    "                           remove_columns = ['age', 'article_id', 'college major',\n",
    "                                             'first_name', 'last_name', 'years_of_journalism',\n",
    "                                             'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061e38a-4e26-4294-8a3f-99f3a51de49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'label', 'token_type_ids'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'label', 'token_type_ids'],\n",
       "        num_rows: 16\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfb51de-f0d2-48d9-9f58-86c46a4b615f",
   "metadata": {},
   "source": [
    "What's this `truncation` argument and this `batched` argument? Let's take a look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08845526-78fb-4096-bb4b-1ca13c7eda12",
   "metadata": {},
   "source": [
    "## An aside on dynamically padded batch size\n",
    "HF has the capacity to dynamically pad your batches such that each input is only as long as any given input in the batch. This helps with memory.You can learn more [here](https://huggingface.co/course/chapter3/2?fw=pt). For now, we'll simply instantiate a data collator and use it during training to demonstrate how we can do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df86dd09-500c-4a14-80c4-32d9a432c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da664a0-b63c-450a-ae74-bab101d55ea1",
   "metadata": {},
   "source": [
    "# 3. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adda4e90-9eae-4dc6-941d-2dedf2a00a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the number of classes\n",
    "no_classes = len(set(demo_ds['train']['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6051da-9e43-48f2-baed-65e34b6488fb",
   "metadata": {},
   "source": [
    "## Define model and task architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606be9d-e38c-48dd-9a8b-73be81ab5d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bert-base-cased'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose the model type and instantiate it for the task\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=no_classes)\n",
    "model.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf580c86-c17e-47d2-b63a-39df13c0d373",
   "metadata": {},
   "source": [
    "## Define settings for basic model training and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aa3713-39b5-4b88-8d41-e2fe41a72ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 16\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.622900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.468300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.458800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=1.516642451286316, metrics={'train_runtime': 32.5202, 'train_samples_per_second': 1.476, 'train_steps_per_second': 0.185, 'total_flos': 2729850728064.0, 'train_loss': 1.516642451286316, 'epoch': 3.0})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set training arguments\n",
    "training_args = TrainingArguments(\"test_trainer\",\n",
    "                                 logging_strategy='epoch')\n",
    "\n",
    "#setup training loop with arguments\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  tokenizer=tokenizer,\n",
    "                  data_collator=data_collator,\n",
    "                  train_dataset=tokenized_ds['train'],\n",
    "                  eval_dataset=tokenized_ds['test'])\n",
    "\n",
    "#train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae4997a-389d-480f-9e0b-ca39f6391399",
   "metadata": {},
   "source": [
    "### Reflect and Discuss\n",
    "* How many epochs of training did this undergo? Why do you think it stopped at this number of epochs?\n",
    "* What if you wanted to train the model more? How do you think you could change the number of epochs?\n",
    "* Practically speaking, how is the model performing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc468b0-0f24-42aa-8642-3e8207394f42",
   "metadata": {},
   "source": [
    "## Training with performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53139608-67f6-417b-9d9c-9218c2f228f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b470e6a08a4d28b904325fff98023d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load a metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "#define the metric behavior\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b28b3-3eaa-438c-92b4-8931485b273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set new training arguments\n",
    "training_args = TrainingArguments(\"test-trainer\",\n",
    "                                  logging_strategy = \"epoch\",\n",
    "                                  evaluation_strategy=\"epoch\")\n",
    "\n",
    "#setup training loop\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "#train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8cc97f-4c52-4147-ba03-b9c2f261632b",
   "metadata": {},
   "source": [
    "### Reflect and Discuss\n",
    "* What new observations are present during model training?\n",
    "* What comments can you make on the performance of the model now?\n",
    "* What metrics are appropriate for your application?\n",
    "* Consider that model training is done in-memory (the model weights are updated in memory, but not returned), and both of our `Trainer`s trained our model `model`. After basic training from Step 9 and training from Step 10, how many epochs has the model been trained?\n",
    "* What are some advantages and disadvantages of the in-memory training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e6f6e-a03b-475f-94a6-5aebc925164e",
   "metadata": {},
   "source": [
    "## A brief aside on performance metrics\n",
    "You may want to use other performance metrics than accuracy. Here are some [metrics available through Huggingface](https://huggingface.co/metrics). If you check out the metrics folder on the [Huggingface datasets](https://github.com/huggingface/datasets) repository, you'll be able to see what's necessary if you need to define another metric. Let's try a different metric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a048a8-8385-4cec-8a65-398ca33c4dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 16\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Fscore</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.111300</td>\n",
       "      <td>1.508415</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>1.593476</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>1.595841</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=0.09966778010129929, metrics={'train_runtime': 36.741, 'train_samples_per_second': 1.306, 'train_steps_per_second': 0.163, 'total_flos': 2729850728064.0, 'train_loss': 0.09966778010129929, 'epoch': 3.0})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    #get predictions by using index of max logit\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    #calculate classification report\n",
    "    perfs = precision_recall_fscore_support(labels, predictions, average='macro', zero_division=0)\n",
    "    perf_dict = dict(zip(['precision', 'recall', 'fscore', 'support'], perfs))\n",
    "    \n",
    "    #return dictionary\n",
    "    return perf_dict\n",
    "\n",
    "#setup training loop\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ba1189-3e5f-42ae-bfae-e79f85a439f5",
   "metadata": {},
   "source": [
    "# 4. Using trained model with `Trainer`\n",
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5bd0ec-ec3f-4f0f-b52f-70c19f6a79b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6205865144729614,\n",
       " 'eval_accuracy': 0.8,\n",
       " 'eval_runtime': 0.368,\n",
       " 'eval_samples_per_second': 40.759,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f6ec1-24fb-43f7-a52a-1cdab4861fc5",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83797f24-bcfb-405d-9f3a-01c8b26fd7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-0.29494038, -0.5705005 ],\n",
       "       [-0.59375405, -0.46901992],\n",
       "       [-0.2581168 , -0.4892554 ],\n",
       "       [-0.20425233, -0.61265355],\n",
       "       [-0.5602172 , -0.5579459 ],\n",
       "       [-0.2200632 , -0.5284079 ],\n",
       "       [-0.55317354, -0.5999937 ],\n",
       "       [-0.4260145 , -0.45691708],\n",
       "       [-0.36270788, -0.3824813 ],\n",
       "       [-0.42646652, -0.30376187],\n",
       "       [-0.37042508, -0.28922018],\n",
       "       [-0.1676437 , -0.5701666 ],\n",
       "       [-0.6241655 , -0.407779  ],\n",
       "       [-0.54866135, -0.4822002 ],\n",
       "       [-0.5887574 , -0.47976074]], dtype=float32), label_ids=array([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1], dtype=int64), metrics={'test_loss': 0.6205865144729614, 'test_accuracy': 0.8, 'test_runtime': 0.3172, 'test_samples_per_second': 47.291})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad2ce4-1806-4bf3-9b68-6afa33356b82",
   "metadata": {},
   "source": [
    "# 5. Sharing your model\n",
    "## `save_model`\n",
    "This will create a model folder with your model weights and all relevant information locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a777f-2b74-43b5-ab8f-b8746165d314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-magazine-classifier\n",
      "Configuration saved in bert-magazine-classifier/config.json\n",
      "Model weights saved in bert-magazine-classifier/pytorch_model.bin\n",
      "tokenizer config file saved in bert-magazine-classifier/tokenizer_config.json\n",
      "Special tokens file saved in bert-magazine-classifier/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('bert-magazine-classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63827fe5-6f3c-4f95-a1a5-ffcdce211fcb",
   "metadata": {},
   "source": [
    "## `push_to_hub`\n",
    "Similarly to datasets, this will push your model to the Huggingface Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0969e67-fb48-4b3a-83e1-fde05bbfee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.push_to_hub('charreaubell/bert-magazine-classifier', private=True, commit_message='initial upload of distilbert magazine classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94abc5f1-076e-4167-9c85-26d77404797f",
   "metadata": {},
   "source": [
    "# 6. Using your fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9238bf-3e4c-4558-99a9-72fbbf328787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file bert-magazine-classifier/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file bert-magazine-classifier/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file bert-magazine-classifier/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at bert-magazine-classifier.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file bert-magazine-classifier/added_tokens.json. We won't load it.\n",
      "loading file bert-magazine-classifier/vocab.txt\n",
      "loading file bert-magazine-classifier/tokenizer.json\n",
      "loading file None\n",
      "loading file bert-magazine-classifier/special_tokens_map.json\n",
      "loading file bert-magazine-classifier/tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_2', 'score': 0.29905053973197937}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create pipeline from your classifier\n",
    "mag_classifier = pipeline('text-classification', model='bert-magazine-classifier')\n",
    "\n",
    "#get output\n",
    "mag_class = mag_classifier('The cat is prettier than any cat I have ever seen.')\n",
    "mag_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
