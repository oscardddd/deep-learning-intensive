{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing your dataset\n",
    "> Getting your files in the right format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we examine how one can work with files and format them into the HuggingFace Datasets format. We'll use some files in this directory on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### What data structure have we been using so far?\n",
    "Recall the [fine-tuning notebook](https://huggingface.co/docs/transformers/training) we looked at earlier. One of the first things that we do is load the data, and however it's done seems to work seamlessly with the rest of the API. Let's look more into this.\n",
    "\n",
    "#### What IS this and why might we use it?\n",
    "Let's learn more about Datasets using the Huggingface [Dataset API](https://huggingface.co/docs/datasets/).\n",
    "\n",
    "#### Questions and Discussion\n",
    "Using pages [The Dataset Object](https://huggingface.co/docs/datasets/access.html) and [Train with Datasets](https://huggingface.co/docs/datasets/use_dataset.html), answer the following questions:\n",
    "* How would you generally describe the structure of a Dataset vs something more like a DataFrame (table)?\n",
    "* What seem to be some operations you can do with a Dataset?\n",
    "* What are the advantages of a Dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Let's try an example.\n",
    "We have some data that we'd like to classify. We've saved our data as follows:\n",
    "* We have one huge directory full of files of data\n",
    "* Each file name is the id of the file\n",
    "* We have another csv file which contains information about all of each of the files, one row for each id\n",
    "\n",
    "_Need to see this visually? Navigate to [the repo](https://github.com/vanderbilt-data-science/deep-learning-intensive) and click on the `workshop-files` directory. This is what the directory looks like._\n",
    "\n",
    "How do we get this in the right format for processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and load the required modules\n",
    "We need to first install `transformers` and `datasets`. Execute the line below if you're using Google Colab. The rest of the modules are already available through Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dl imports\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset, Dataset, ClassLabel, load_from_disk, DatasetDict\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "#import data science packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#import file helper packages\n",
    "import glob\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "Our data is structured as a table of information about authors including the ids of articles they've written, and a set of data files (on GitHub) named with the ID of each of the texts. We'll leverage this here just for an example, and build up a table.\n",
    "\n",
    "#### Get data info table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set base_url\n",
    "base_url = 'https://raw.githubusercontent.com/vanderbilt-data-science/deep-learning-intensive/master/workshop-files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#article data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add data (note: this could be a URL path instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make into HuggingFace Dataset\n",
    "Let's figure out some different ways that we can load the data. Let's learn more from the [Load documentation reference](https://huggingface.co/docs/datasets/loading.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions and Discussion\n",
    "Datasets tend to come in one of two forms: canonical and community. Let's [explore this further](https://huggingface.co/docs/datasets/share.html).\n",
    "\n",
    "This was just one of _many_ ways that you can create a dataset. Later on, we'll push this to the hub using a simple command. What kind of data do you have and in what format? Join the breakout rooms below based on the situation that best fits your scenario, and discuss with your group how you think you need to organize your data and the steps for uploading your data. Some resources are noted below.\n",
    "1. **Programmatic standard formats:**\n",
    "    * e.g., Datasets in CSV, JSON, Parquet, or Text formats stored locally or remotely\n",
    "    * Use the load documentation references above. Using the [Structure Your Repository](https://huggingface.co/docs/datasets/repository_structure.html) resource, define how your repo should be structured.\n",
    "2. **Low-code standard formats:**\n",
    "    * e.g., Datasets in CSV, JSON, Parquet, or Text formats that you want to be hosted on HuggingFace Hub\n",
    "    * Explore the [Huggingface Hub direct upload](https://huggingface.co/docs/datasets/upload_dataset.html) reference. Using the [Structure Your Repository](https://huggingface.co/docs/datasets/repository_structure.html) resource, define how your data repo should be structured. If you're interested in command-line programming, explore the [Share](https://huggingface.co/docs/datasets/share.html) code reference for the terminal/command line equivalent of the direct upload.\n",
    "3. **Non-standard formatted resources that are referenced by URL:**\n",
    "    * e.g., Large image or audio datasets to be loaded via URL, or datasets that need custom operations\n",
    "    * Explore the use of [dataset loading scripts](https://huggingface.co/docs/datasets/dataset_script.html). It may be extremely helpful to look at how a dataset of this type with a loading script is structured and the usage of the Python script. Check out the [Huggingface internal image test dataset](https://huggingface.co/datasets/hf-internal-testing/fixtures_image_utils) to see an example. Make sure to click on the _Files and Versions_ tab and the .py file to see the loading script. \n",
    "4. **Datasets you want to have multiple configurations:**\n",
    "    * e.g., T0, training on multiple objectives/tasks\n",
    "    * Explore the use of [dataset loading scripts](https://huggingface.co/docs/datasets/dataset_script.html). It may be instructive to check out the given GLUE example for multiple configurations. A quick reference to the repo loading script is [on GitHub.](https://github.com/huggingface/datasets/blob/master/datasets/super_glue/super_glue.py)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations on HuggingFace Datasets\n",
    "Let's explore some operations that we can do on Datasets. We can learn more on the [Process](https://huggingface.co/docs/datasets/process.html) page here. Note that if you're using audio data, the [Process Audio](https://huggingface.co/docs/datasets/audio_process.html) reference has lots of helpful functions for processing audio.\n",
    "## `features`\n",
    "View detailed information about the features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `rename_column`\n",
    "Sometimes, we need to rename columns (obviously!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `class_encode_column`\n",
    "Interestingly enough, there is no sense of the labels or target class here.  We can add this by changing one Indicate label columns by encoding the columns to a class type. Essentially, this changes this columnt to integer values with a FeatureLabel type, meaning that there are dictionary lookups for id2label and label2id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `map`\n",
    "Familiar with map functions? The pandas apply function? Sometimes we need to apply a function to each of the examples to create or update fields. We can do this here with the Dataset `map` function.\n",
    "\n",
    "In this example, we take an extra difficult approach to creating a column with the real names of the labels. We'll use some advantages of the `ClassLabel` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "Sometimes, we want to see one or more examples from the data. We can use normal indexing approaches to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID conversion\n",
    "We often need to convert our string labels back and forth to/from integers. We can do this using methods of the `ClassLabel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train_test_split`\n",
    "We need to split our data into minimally a train/test set. We often prefer a train/validation/test split to evaluate performance of the model independently even during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with train/test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training into train and validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update original ds with re-split training and validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharing your dataset\n",
    "## `save_to_disk`\n",
    "Want to save your dataset and load it later? You can directly use the save functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `load_from_disk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `push_to_hub`\n",
    "Want to directly upload your dataset to Huggingface now that you've got it created? You can programmatically push it to the Hub. This requires that you're signed into your HF account. You may need to uncomment and run the following line to help you store your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git config credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you input your Huggingface _token_ below. If this doesn't work, you can use the `token` parameter in `push_to_hub` and set it a string of your Huggingface token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on pushing to the hub\n",
    "Instead of pushing datasets, you can also push the raw data (we'll use a pandas dataframe here to parquet format). Let's see how we can do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo, Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/datasets/charreaubell/demo_data_raw'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uncomment to create a remote repo\n",
    "repo_url = create_repo(name=\"demo_data_raw\", organization='charreaubell', repo_type=\"dataset\", private=True)\n",
    "repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/datasets/charreaubell/demo_data_raw into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "#clone the repo locally\n",
    "repo = Repository(local_dir=\"demo_data_raw\", clone_from='charreaubell/demo_data_raw', repo_type='dataset', use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31516"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the data\n",
    "data_ds['train'].to_parquet('demo_data_raw/demo_data_train.parquet')\n",
    "data_ds['test'].to_parquet('demo_data_raw/demo_data_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7c73c150f2490889b0237db63f2f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file demo_data_train.parquet: 100%|##########| 8.53k/8.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9655469c0e5e4da39cd4461524cc636b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file demo_data_test.parquet: 100%|##########| 5.77k/5.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/datasets/charreaubell/demo_data_raw\n",
      "   4bbe4e5..8799829  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/datasets/charreaubell/demo_data_raw/commit/8799829ce46f5278934337af2393ab0b8986a2c9'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#push to hub\n",
    "is_done = repo.push_to_hub(commit_message='train and test raw data push')\n",
    "is_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions and Discussion\n",
    "Using the Datasets reference:\n",
    "* Tell me about some of the functions offered for interfacing with Cloud Storage\n",
    "* How could you stream live data or extremely large data?\n",
    "* Look at the Search Index page. Describe the general functionality offered here. How might you use it in your application?\n",
    "* What else seems cool to you?\n",
    "\n",
    "# What we've covered\n",
    "We've covered a lot of ground today!  We've discussed several things:\n",
    "* Explored HuggingFace Datasets API\n",
    "* Learned about Dataset structure\n",
    "* Conversion from data to HuggingFace Dataset structure\n",
    "* Uploading to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework assignment\n",
    "Create a HuggingFace dataset from your data, and upload it to the HuggingFace Hub!  We'll be working on your own data for the next few classes. If you're unable to upload your data, make sure you know how to access it programmatically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
